\documentclass[]{article}

\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{graphicx}

\graphicspath{ {.} }

\setlength{\parindent}{0pt}

\title{Operating Systems Honor Track - Lab 2 Report}
\author{叶高翔 2100012801}
\date{2023.10.21}

\begin{document}

\maketitle

For the challenge, I choose to use the \verb|PTE_PS| bit to implement 4MB pages to map physical address to virtual address above \verb|KERNBASE|.

\section*{Exercise 1}

In \verb|boot_alloc()|, we solely manipulate \verb|nextfree| to allocate memory. For each of the function, please see comments within the function for more details on its implementation.

\begin{verbatim}
static void *
boot_alloc(uint32_t n)
{
	static char *nextfree;	// virtual address of next byte of free memory
	char *result;

	// Initialize nextfree if this is the first time.
	// 'end' is a magic symbol automatically generated by the linker,
	// which points to the end of the kernel's bss segment:
	// the first virtual address that the linker did *not* assign
	// to any kernel code or global variables.
	if (!nextfree) {
		extern char end[];
		nextfree = ROUNDUP((char *) end, PGSIZE);
	}

	// Allocate a chunk large enough to hold 'n' bytes, then update
	// nextfree.  Make sure nextfree is kept aligned
	// to a multiple of PGSIZE.
	//
	// LAB 2: Your code here.
	// TODO - Done
	if (n < 0)
		panic("n < 0 in boot_alloc\n");

	result = nextfree;

	if (((uint32_t) nextfree - KERNBASE) > (npages * PGSIZE))
		panic("Out of memory in boot_alloc\n");

	if (n > 0)
		nextfree = ROUNDUP(nextfree+n, PGSIZE);

	return result;
}
\end{verbatim}

In \verb|mem_init()|, we use \verb|boot_alloc()| to allocate space for the kernel page directory and the pages data structure, and initialize them.

\begin{verbatim}
void
mem_init(void)
{
  uint32_t cr0;
  size_t n;

  // Find out how much memory the machine has (npages & npages_basemem).
  i386_detect_memory();

  // Remove this line when you're ready to test this function.
  // TODO
  // panic("mem_init: This function is not finished\n");

  //////////////////////////////////////////////////////////////////////
  // create initial page directory.
  kern_pgdir = (pde_t *) boot_alloc(PGSIZE);
  memset(kern_pgdir, 0, PGSIZE);

  //////////////////////////////////////////////////////////////////////
  // Recursively insert PD in itself as a page table, to form
  // a virtual page table at virtual address UVPT.
  // (For now, you don't have understand the greater purpose of the
  // following line.)

  // Permissions: kernel R, user R
  kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P;

  //////////////////////////////////////////////////////////////////////
  // Allocate an array of npages 'struct PageInfo's and store it in 'pages'.
  // The kernel uses this array to keep track of physical pages: for
  // each physical page, there is a corresponding struct PageInfo in this
  // array.  'npages' is the number of physical pages in memory.  Use memset
  // to initialize all fields of each struct PageInfo to 0.
  // Your code goes here:
  // TODO - Done
  pages = boot_alloc(sizeof(struct PageInfo) * npages);
  memset((void *)pages, 0, sizeof(struct PageInfo) * npages);

  //////////////////////////////////////////////////////////////////////
  // Now that we've allocated the initial kernel data structures, we set
  // up the list of free physical pages. Once we've done so, all further
  // memory management will go through the page_* functions. In
  // particular, we can now map memory using boot_map_region
  // or page_insert
  page_init();
  ...
\end{verbatim}

In \verb|page_init()|, apart from regions of the low memory that we are instructed to mark as occupied, we use \verb|boot_alloc(0)| to figure out the first byte of the extended memory that is free, and mark the pages before it as occupied and those after it as free.

\begin{verbatim}
void
page_init(void)
{
  // The example code here marks all physical pages as free.
  // However this is not truly the case.  What memory is free?
  //  1) Mark physical page 0 as in use.
  //     This way we preserve the real-mode IDT and BIOS structures
  //     in case we ever need them.  (Currently we don't, but...)
  //  2) The rest of base memory, [PGSIZE, npages_basemem * PGSIZE)
  //     is free.
  //  3) Then comes the IO hole [IOPHYSMEM, EXTPHYSMEM), which must
  //     never be allocated.
  //  4) Then extended memory [EXTPHYSMEM, ...).
  //     Some of it is in use, some is free. Where is the kernel
  //     in physical memory?  Which pages are already in use for
  //     page tables and other data structures?
  //
  // Change the code to reflect this.
  // free pages!
  // TODO - Done

  size_t i;
  page_free_list = NULL;	// This signifies the end of the free page linked list

  // Page 0 is reserved
  pages[0].pp_ref = 1;
  pages[0].pp_link = NULL;

  for (i = 1; i*PGSIZE < IOPHYSMEM; i++) {	// Other parts of low memory is free
    pages[i].pp_ref = 0;
    pages[i].pp_link = page_free_list;
    page_free_list = &pages[i];
  }

  for (; i*PGSIZE < EXTPHYSMEM; i++) {	// IO hole reserved
    pages[i].pp_ref = 1;
    pages[i].pp_link = NULL;
  }

  // In physical memory, the kernel text and the data structures we have
  // allocated are contiguous and grows from EXPHYSMEM up.
  // We can use boot_alloc(0) to find the end of the allocated part in
  // extended memory. However, note that this returns a virtual address
  // (above KERNBASE?), which we need to convert to physical address.
  uint32_t paddr_extmem_free_start = PADDR(boot_alloc(0));
  for (; i*PGSIZE < paddr_extmem_free_start; i++) { // Used ext mem
    pages[i].pp_ref = 1;
    pages[i].pp_link = NULL;
  }

  for (; i < npages; i++) {  // Rest of extended memory is free
    pages[i].pp_ref = 0;
    pages[i].pp_link = page_free_list;
    page_free_list = &pages[i];
  }
}
\end{verbatim}

We manipulate the free page linked list in \verb|page_alloc()|:

\begin{verbatim}
struct PageInfo *
page_alloc(int alloc_flags)
{	
  if (page_free_list == NULL)
		return NULL;

	struct PageInfo *allocated_page = page_free_list;
	page_free_list = allocated_page->pp_link;
	allocated_page->pp_link = NULL;

	if (alloc_flags & ALLOC_ZERO)
		memset(page2kva(allocated_page), 0, PGSIZE);

	return allocated_page;
}
\end{verbatim}

\verb|page_free()| as follows:

\begin{verbatim}
void
page_free(struct PageInfo *pp)
{
  // Fill this function in
  // Hint: You may want to panic if pp->pp_ref is nonzero or
  // pp->pp_link is not NULL.
  // TODO - Done
 
  if (pp->pp_ref)
    panic("To-be-freed page's pp_ref is nonzero in page_free\n");
  if (pp->pp_link)
    panic("To-be-freed page's pp_link is not NULL in page_free\n");
 
  pp->pp_link = page_free_list;
  page_free_list = pp;
}
\end{verbatim}

\section*{Question 1}
\verb|value| is a C pointer pointing to a \verb|char| of 10. As all C pointers are virtual addresses, \verb|value| can only be cast to a virtual address, in other words, \verb|mystery_t| is \verb|uintptr_t|.

\section*{Question 2}
By running \verb|info pg| in the QEMU console, we can get the following output:
\begin{verbatim}
(qemu) info pg
VPN range     Entry         Flags        Physical page
[ef000-ef3ff]  PDE[3bc]     -------UWP
  [ef000-ef3ff]  PTE[000-3ff] -------U-P 0011c-0051b
[ef400-ef7ff]  PDE[3bd]     -------U-P
  [ef7bc-ef7bc]  PTE[3bc]     -------UWP 003fd
  [ef7bd-ef7bd]  PTE[3bd]     -------U-P 0011b
  [ef7bf-ef7bf]  PTE[3bf]     -------UWP 003fe
  [ef7c0-ef7df]  PTE[3c0-3df] ----A--UWP 003ff 003fc 003fb 003fa 003f9 003f8 ..
  [ef7e0-ef7ff]  PTE[3e0-3ff] -------UWP 003dd 003dc 003db 003da 003d9 003d8 ..
[efc00-effff]  PDE[3bf]     -------UWP
  [efff8-effff]  PTE[3f8-3ff] --------WP 0010f-00116
[f0000-f03ff]  PDE[3c0]     ----A--UWP
  [f0000-f0000]  PTE[000]     --------WP 00000
  [f0001-f009f]  PTE[001-09f] ---DA---WP 00001-0009f
  [f00a0-f00b7]  PTE[0a0-0b7] --------WP 000a0-000b7
  [f00b8-f00b8]  PTE[0b8]     ---DA---WP 000b8
  [f00b9-f00ff]  PTE[0b9-0ff] --------WP 000b9-000ff
  [f0100-f0105]  PTE[100-105] ----A---WP 00100-00105
  [f0106-f0115]  PTE[106-115] --------WP 00106-00115
  [f0116-f0116]  PTE[116]     ---DA---WP 00116
  [f0117-f0119]  PTE[117-119] --------WP 00117-00119
  [f011a-f011b]  PTE[11a-11b] ---DA---WP 0011a-0011b
  [f011c-f011c]  PTE[11c]     ----A---WP 0011c
  [f011d-f011d]  PTE[11d]     ---DA---WP 0011d
  [f011e-f015b]  PTE[11e-15b] ----A---WP 0011e-0015b
  [f015c-f03bd]  PTE[15c-3bd] ---DA---WP 0015c-003bd
  [f03be-f03ff]  PTE[3be-3ff] --------WP 003be-003ff
[f0400-f7fff]  PDE[3c1-3df] ----A--UWP
  [f0400-f7fff]  PTE[000-3ff] ---DA---WP 00400-07fff
[f8000-fffff]  PDE[3e0-3ff] -------UWP
  [f8000-fffff]  PTE[000-3ff] --------WP 08000-0ffff
\end{verbatim}

With this information, we can fill out the  following table:

\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Entry & Base Virtual Address & Points to (logically): \\
		\hline
		1023 & \verb|0xfffff000| & Page table for top 4MB of phys memory\\
		\hline
		... & & \\
		\hline
		960 & \verb|0xf0000000| & Page table for first 4MB of phys memory\\
		\hline
		959 & \verb|0xefc00000| & Page table for kernel stack \\
		\hline
		957 & \verb|0xef400000| & Virtual Page Table\\
		\hline
		956 & \verb|0xef000000| & Page table for read-only pages\\
		\hline
	\end{tabular}
\end{center}

\section*{Question 3}
The kernel's memory is protected from user programs by 3 mechanisms.
\begin{enumerate}
	\item The protection bit in the page directory/table entries prevent user programs from writing to, and sometimes reading from kernel memory.
	\item Invalid space exists under the kernel stack, attempting to writing into which will immediately cause a fault.
	\item The bottom of the kernel stack is occupied by a guard page, attempting to writing into which will also cause a fault.
\end{enumerate}

\section*{Question 4}
Looking at the memory initialization process in \verb|mem_init()|, the maximum physical size of memory is bottlenecked by the maximum allowed size of \verb|pages| in \verb|pages = boot_alloc(sizeof(struct PageInfo) * npages);|. Before executing the \verb|boot_alloc()| function in this line of code, \verb|nextfree = 0xf011c000|; and our rudimentary page table in \verb|entrypgdir.c| maps at most 4MB of physical memory, thus the maximum size of \verb|pages| is $\mathrm{0xf0400000} - \mathrm{0xf011c000} = 2960\text{KB}$, which can hold $2960 \times 1024/8\times4\text{KB} = 1480\text{MB}$ of physical memory.

\section*{Question 5}

The \verb|pages| data structure would take up 2960KB, the page directory would take up 4KB, and if every possible page table is created, they would take up $4096/4*4KB = 4MB$. Combining these three gets us 7060KB.

\section*{Question 6}
We transition to running at an EIP above KERNBASE after \verb|jmp	*%eax|. The ability to execute at a low EIP after enabling paging is enabled by \verb|entrypgdir.c|, which defines a mapping from virtual address [KERNBASE, KERNBASE+4MB) to physical addresses [0, 4MB) as well as one from [0, 4MB) to [0, 4MB). This transition to a higher EIP is necessary as once \verb|kern_pgdir| is enabled, the [0, 4MB) to [0, 4MB) mapping no longer holds; and that the address above 1MB is not designated for kernel text.

\section*{Exercise 4}

In \verb|pgdir_walk()|, we need to keep track of which addresses are linear and which ones are physical.

\begin{verbatim}
pte_t *
pgdir_walk(pde_t *pgdir, const void *va, int create)
{
  // Fill this function in
  // TODO - Done

  pde_t *pde_addr = &(pgdir[PDX(va)]);

  physaddr_t pt_addr = PTE_ADDR(*pde_addr);
  
  if (*pde_addr & PTE_P) {	// Respective page table is present
    physaddr_t pte_addr = pt_addr + PTX(va)*4;
    return (pte_t *)KADDR(pte_addr);	// Returns a virtual address

  } else if (create) {	// Respective page table is absent
    struct PageInfo *new_page = page_alloc(0);
    if (new_page == NULL)
      return NULL;

    new_page->pp_ref++;
    memset(page2kva(new_page), 0, PGSIZE);

    // Update the page directory entry
    *pde_addr = page2pa(new_page) | PTE_P | PTE_W | PTE_U;

    return (pte_t *)KADDR(page2pa(new_page) + PTX(va)*4);

  } else
    return NULL;
}
\end{verbatim}

\verb|boot_map_region()| finds each page corresponding to the linear address in the specified region, and sets their pde to the corresponding physical address and permission bits.

\begin{verbatim}
static void
boot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm)
{
  // Fill this function in
  // TODO - Done

  for (size_t i = 0; i < size / PGSIZE; i++) {
    pte_t *pte_addr = pgdir_walk(pgdir, (void *)(va + PGSIZE*i), 1);
    if (pte_addr == NULL)
      panic("Failed to create new page table in pgdir_walk in boot_map_region\n");
    *pte_addr = (pa+PGSIZE*i) | (perm | PTE_P);
  }
}
\end{verbatim}

\verb|page_lookup()| utilizes \verb|pgdir_walk()|:

\begin{verbatim}
struct PageInfo *
page_lookup(pde_t *pgdir, void *va, pte_t **pte_store)
{
  // Fill this function in
  // TODO - Done
  
  pte_t *pte_addr = pgdir_walk(pgdir, va, 0);
  if (pte_addr == NULL) return NULL;
  if (pte_store) *pte_store = pte_addr;
  return pa2page(PTE_ADDR(*pte_addr));
}
\end{verbatim}

We fill in \verb|page_remove()| according to the directions. Note that casting \verb|pte_store| directly from a non-zero number will cause a triple fault.

\begin{verbatim}
void
page_remove(pde_t *pgdir, void *va)
{
  // Fill this function in
  // TODO - Done

  pte_t pte_store_0 = 0;
  pte_t *pte_store_1 = &pte_store_0;
  pte_t **pte_store = &pte_store_1; // Initialize it with really anything

  struct PageInfo *pp = page_lookup(pgdir, va, pte_store);
  if (pp == NULL) {
    return;
  }

  page_decref(pp);
  **pte_store = (uint32_t)0;	// Clear corresponding pte
  tlb_invalidate(pgdir, va); 
  return;
}
\end{verbatim}

The subtle bug mentioned in the directions is that if we increment the \verb|pp|'s \verb|pp_ref| after it was inserted, then if the same pp with a \verb|pp_ref = 1| is reinserted, its \verb|pp_ref| is first decremented to 0 and thus freed. As a solution, we increment \verb|pp->ref| first.

\begin{verbatim}
int
page_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm)
{
  // Fill this function in
  // TODO - Done
  
  pte_t *pte_addr = pgdir_walk(pgdir, va, 1);
  if (pte_addr == NULL)
    return -E_NO_MEM;

  if (*pte_addr & PTE_P) {	// There's already a page present at va
    // Note that if the same pp with a pp_ref = 1 is reinserted,
    // its pp_ref is decremented to 0 and thus freed.
    // As a solution, increment pp->ref first.
    pp->pp_ref++;
    page_remove(pgdir, va);
    *pte_addr = page2pa(pp) | (perm | PTE_P);
    tlb_invalidate(pgdir, va);
    return 0;

  } else {	// No page at va
    pp->pp_ref++;
    *pte_addr = page2pa(pp) | (perm | PTE_P);
    return 0;
  }
}
\end{verbatim}

\section*{Exercise 5}

For the three spots to be filled in, we respectively fill in:

\begin{verbatim}
boot_map_region(kern_pgdir, UPAGES, PTSIZE, PADDR(pages), PTE_U | PTE_P);
boot_map_region(kern_pgdir, KSTACKTOP-KSTKSIZE, KSTKSIZE, PADDR(bootstack), 
PTE_P|PTE_W);
boot_map_region(kern_pgdir, KERNBASE, (2^32)-KERNBASE, 0, PTE_P|PTE_W);
\end{verbatim}

Note that for the kernel stack, we do not need to map the guard page to any physical memory.

\section*{Challenge}
This challenge can be broken down into parts:
\begin{enumerate}
  \item Check machine support for variable page size(in technical terms, PSE(page size extension)).
  \item Change the \verb|boot_map_region()| and \verb|pgdir_walk()| function to make variable page size possible.
  \item Change existing checks to accommodate for 4MB pages.
  \item Change the \verb|cr4| register to enable PSE.
\end{enumerate}

\subsection*{Check Machine Support}
We use the existing \verb|cpuid| function. When passing an argument of 1 through the eax register, the edx register's bit 3 returns PSE availability. We then store this information in a file-scoped variable.

\begin{verbatim}
static void
check_pse_availability(void)
{
  uint32_t eax, ebx, ecx, edx;
  cpuid(1, &eax, &ebx, &ecx, &edx);
  support_pse = edx & 0x8;
  if (support_pse)
    cprintf("System supports PSE\n");
  else
    cprintf("System does not support PSE\n");
}
\end{verbatim}

\subsection*{Change function definition}

We modify the two functions to:

\begin{verbatim}
static void
boot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa,
int perm, int use_pse)
{
  // Fill this function in
  // TODO - Done

  if (use_pse && support_pse){	// Use 4MB pages
    cprintf("Using 4MB pages in boot_map_region()\n");

    for (size_t i = 0; i < size / LPGSIZE; i++) {
      cprintf("Page %d\n", i);
      // cprintf("size = %d\n", size);
      // cprintf("LPGSIZE = %d\n", LPGSIZE);
      // cprintf("i = %d, cap = %d\n", i, size / LPGSIZE);
      pte_t *pde_addr = pgdir_walk(pgdir, (void *)(va + LPGSIZE*i), 1, 1);
      cprintf("pde_addr: %p\n", pde_addr);
      if (pde_addr == NULL)
        panic("pgdir_walk() returns NULL when mapping a 4MB page 
        in boot_map_region()\n");

      // cprintf("Setting pde for %d-th 4MB page\n", i);
      cprintf("physical address: %p\n", pa+LPGSIZE*i);
      cprintf("virtual address: %p\n", va + LPGSIZE*i);
      *pde_addr = (pa+LPGSIZE*i) | (perm | PTE_P | PTE_PS);
      cprintf("Contents of pde: %p\n", *pde_addr);
    }

  } else {	// Use 4KB pages
    if (use_pse)
      cprintf("boot_map_region wants to use 4MB pages but 4MB pages not 
      supported by system, falling back to 4KB pages\n");

    for (size_t i = 0; i < size / PGSIZE; i++) {
      pte_t *pte_addr = pgdir_walk(pgdir, (void *)(va + PGSIZE*i), 1, 0);
      if (pte_addr == NULL)
        panic("Failed to create new page table in pgdir_walk in boot_map_region\n");

      *pte_addr = (pa+PGSIZE*i) | (perm | PTE_P);
    }
  }
}
\end{verbatim}

\begin{verbatim}
pte_t *
pgdir_walk(pde_t *pgdir, const void *va, int create, int use_pse)
{
  // Fill this function in
  // TODO - Done

  pde_t *pde_addr = &(pgdir[PDX(va)]);

  if (use_pse) {
    // cprintf("Using 4MB page in pgdir_walk()\n");
    return pde_addr;
  }

  physaddr_t pt_addr = PTE_ADDR(*pde_addr);
  
  if (*pde_addr & PTE_P) {	// Respective page table is present
    physaddr_t pte_addr = pt_addr + PTX(va)*4;
    return (pte_t *)KADDR(pte_addr);	// Returns a virtual address

  } else if (create) {	// Respective page table is absent
    struct PageInfo *new_page = page_alloc(0);
    if (new_page == NULL)
      return NULL;

    new_page->pp_ref++;
    memset(page2kva(new_page), 0, PGSIZE);

    // Update the page directory entry
    *pde_addr = page2pa(new_page) | PTE_P | PTE_W | PTE_U;

    return (pte_t *)KADDR(page2pa(new_page) + PTX(va)*4);

  } else
    return NULL;
}
\end{verbatim}

Note that we changed their interface too, so all calls to them need to change too.

\subsection*{Check changes}

We change \verb|check_va2pa()| as follow:

\begin{verbatim}
static physaddr_t
check_va2pa(pde_t *pgdir, uintptr_t va)
{
  pte_t *p;

  pgdir = &pgdir[PDX(va)];	// va of the page directory entry
  if (!(*pgdir & PTE_P)) {	// If page table/4MB page is not present
    cprintf("Case 0 in check_va2pa()\n");
    return ~0;
  }

  if (*pgdir & PTE_PS)
    return PSE_PTE_ADDR(*pgdir);

  p = (pte_t*) KADDR(PTE_ADDR(*pgdir)); // va of the page table entry
  if (!(p[PTX(va)] & PTE_P)) {	// If page not present
    cprintf("Case 1 in check_va2pa()\n");
    return ~0;
  }
  return PTE_ADDR(p[PTX(va)]);
}
\end{verbatim}

\subsection*{Flag}

Finally, we set cr4 to enable PSE:

\begin{verbatim}
if (support_pse)
	lcr4(rcr4()|CR4_PSE);
\end{verbatim}

\end{document}